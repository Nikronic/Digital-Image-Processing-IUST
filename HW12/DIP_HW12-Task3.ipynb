{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rd274HEd8Mg5",
    "outputId": "084b34ce-22b1-4c3f-854c-7db11bff9642"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " %tensorflow_version 1.x \n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.engine.topology import InputSpec\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D, Reshape\n",
    "from keras.layers import Conv2D, Activation, Input, MaxPooling2D, ZeroPadding2D\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Conv2DNormalization(Layer):\n",
    "    def __init__(self, scale, **kwargs):\n",
    "        self.axis = 3\n",
    "        self.scale = scale\n",
    "        super(Conv2DNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "        init_gamma = self.scale * np.ones(shape)\n",
    "        self.gamma = K.variable(init_gamma, name='{}_gamma'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.l2_normalize(x, self.axis)\n",
    "        output = output * self.gamma\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEyx-cfkDolM"
   },
   "outputs": [],
   "source": [
    "def SSD(input_shape=(300, 300, 3),\n",
    "        n_classes=3,\n",
    "        n_boxes=[4, 6, 6, 6, 4, 4],\n",
    "        weights=None,\n",
    "        l2_regularization=0.0005):\n",
    "    \"\"\"\n",
    "    Define the SSD model\n",
    "\n",
    "    :param input_shape: Input image shape\n",
    "    :param n_classes: number of output classes (object types)\n",
    "    :param n_boxes: number of predefined anchor boxes in each SSD feature extractor layer\n",
    "    :param l2_regularizaion: Regularization term hyperparameter\n",
    "    \"\"\"\n",
    "    \n",
    "    x = Input(shape=input_shape)\n",
    "\n",
    "    ##### Start - base model - Vgg16 #####\n",
    "    conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv1_1')(x)\n",
    "    conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv1_2')(conv1_1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool1')(conv1_2)\n",
    "\n",
    "\n",
    "    conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv2_1')(pool1)\n",
    "    conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv2_2')(conv2_1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool2')(conv2_2)\n",
    "\n",
    "\n",
    "    conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv3_1')(pool2)\n",
    "    conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv3_2')(conv3_1)\n",
    "    conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv3_3')(conv3_2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool3')(conv3_3)\n",
    "\n",
    "\n",
    "    conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv4_1')(pool3)\n",
    "    conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv4_2')(conv4_1)\n",
    "    conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv4_3')(conv4_2)\n",
    "    conv4_3_norm = Conv2DNormalization(20, name='conv4_3_norm')(conv4_3)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same', name='pool4')(conv4_3)\n",
    "\n",
    "\n",
    "\n",
    "    conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv5_1')(pool4)\n",
    "    conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv5_2')(conv5_1)\n",
    "    conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv5_3')(conv5_2)\n",
    "    pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same', name='pool5')(conv5_3)    \n",
    "    ##### End - base model - Vgg16 #####\n",
    "    \n",
    "    fc6 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=l2(l2_regularization), name='fc6')(pool5)\n",
    "    fc7 = Conv2D(1024, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=l2(l2_regularization), name='fc7')(fc6)\n",
    "    \n",
    "\n",
    "    conv6_1 = Conv2D(256, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv6_1')(fc7)\n",
    "    conv6_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv6_padding')(conv6_1)\n",
    "    conv6_2 = Conv2D(512, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv6_2')(conv6_1)\n",
    "\n",
    "\n",
    "    conv7_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv7_1')(conv6_2)\n",
    "    conv7_1 = ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv7_padding')(conv7_1)\n",
    "    conv7_2 = Conv2D(256, (3, 3), strides=(2, 2), activation='relu', padding='valid', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv7_2')(conv7_1)\n",
    "\n",
    "\n",
    "    conv8_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv8_1')(conv7_2)\n",
    "    conv8_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv8_2')(conv8_1)\n",
    "\n",
    "\n",
    "    conv9_1 = Conv2D(128, (1, 1), activation='relu', padding='same', kernel_initializer='he_normal', \n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv9_1')(conv8_2)\n",
    "    conv9_2 = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='valid', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(l2_regularization), name='conv9_2')(conv9_1)\n",
    "\n",
    "\n",
    "    # confidence predictions\n",
    "    conv4_3_norm_conf = Conv2D(n_boxes[0] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv4_3_norm_conf')(conv4_3_norm)\n",
    "    fc7_conf = Conv2D(n_boxes[1] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='fc7_conf')(fc7)\n",
    "    conv6_2_conf = Conv2D(n_boxes[2] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv6_2_conf')(conv6_2)\n",
    "    conv7_2_conf = Conv2D(n_boxes[3] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv7_2_conf')(conv7_2)\n",
    "    conv8_2_conf = Conv2D(n_boxes[4] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv8_2_conf')(conv8_2)\n",
    "    conv9_2_conf = Conv2D(n_boxes[5] * n_classes, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv9_2_conf')(conv9_2)\n",
    "    \n",
    "\n",
    "    # location predictions\n",
    "    conv4_3_norm_loc = Conv2D(n_boxes[0] * 4, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv4_3_norm_loc')(conv4_3_norm)\n",
    "    fc7_loc = Conv2D(n_boxes[1] * 4, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='fc7_loc')(fc7)\n",
    "    conv6_2_loc = Conv2D(n_boxes[2] * 4, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv6_2_loc')(conv6_2)\n",
    "    conv7_2_loc = Conv2D(n_boxes[3] * 4, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv7_2_loc')(conv7_2)\n",
    "    conv8_2_loc = Conv2D(n_boxes[4] * 4, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv8_2_loc')(conv8_2)\n",
    "    conv9_2_loc = Conv2D(n_boxes[5] * 4, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=l2(l2_regularization), name='conv9_2_loc')(conv9_2)\n",
    "\n",
    "    # Flatten non-class axises of confidence predictions\n",
    "    conv4_3_norm_conf_reshape = Reshape((-1, n_classes), name='conv4_3_norm_conf_reshape')(conv4_3_norm_conf)\n",
    "    fc7_conf_reshape = Reshape((-1, n_classes), name='fc7_conf_reshape')(fc7_conf)\n",
    "    conv6_2_conf_reshape = Reshape((-1, n_classes), name='conv6_2_conf_reshape')(conv6_2_conf)\n",
    "    conv7_2_conf_reshape = Reshape((-1, n_classes), name='conv7_2_conf_reshape')(conv7_2_conf)\n",
    "    conv8_2_conf_reshape = Reshape((-1, n_classes), name='conv8_2_conf_reshape')(conv8_2_conf)\n",
    "    conv9_2_conf_reshape = Reshape((-1, n_classes), name='conv9_2_conf_reshape')(conv9_2_conf)\n",
    "\n",
    "    # Flatten non-location axises of location predictions\n",
    "    conv4_3_norm_loc_reshape = Reshape((-1, 4), name='conv4_3_norm_loc_reshape')(conv4_3_norm_loc)\n",
    "    fc7_loc_reshape = Reshape((-1, 4), name='fc7_loc_reshape')(fc7_loc)\n",
    "    conv6_2_loc_reshape = Reshape((-1, 4), name='conv6_2_loc_reshape')(conv6_2_loc)\n",
    "    conv7_2_loc_reshape = Reshape((-1, 4), name='conv7_2_loc_reshape')(conv7_2_loc)\n",
    "    conv8_2_loc_reshape = Reshape((-1, 4), name='conv8_2_loc_reshape')(conv8_2_loc)\n",
    "    conv9_2_loc_reshape = Reshape((-1, 4), name='conv9_2_loc_reshape')(conv9_2_loc)\n",
    "\n",
    "    # Concatenate non-class axises\n",
    "    conf = concatenate([conv4_3_norm_conf_reshape,\n",
    "                             fc7_conf_reshape,\n",
    "                             conv6_2_conf_reshape,\n",
    "                             conv7_2_conf_reshape,\n",
    "                             conv8_2_conf_reshape,\n",
    "                             conv9_2_conf_reshape], axis=1, name='conf')\n",
    "\n",
    "    # Concatenate non-location axises\n",
    "    loc = concatenate([conv4_3_norm_loc_reshape,\n",
    "                            fc7_loc_reshape,\n",
    "                            conv6_2_loc_reshape,\n",
    "                            conv7_2_loc_reshape,\n",
    "                            conv8_2_loc_reshape,\n",
    "                            conv9_2_loc_reshape], axis=1, name='loc')\n",
    "\n",
    "    conf_softmax = Activation('softmax', name='conf_softmax')(conf)\n",
    "    predictions = concatenate([loc, conf_softmax], axis=2, name='predictions')\n",
    "\n",
    "\n",
    "    model = Model(inputs=x, outputs=predictions)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights, by_name=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "29bvDfrtwbCR"
   },
   "outputs": [],
   "source": [
    "class MultiboxLoss(object):\n",
    "    def __init__(self, num_classes, neg_pos_ratio=3,\n",
    "                 alpha=1.0, background_id=0, max_num_negatives=300):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.background_id = background_id\n",
    "        self.max_num_negatives = max_num_negatives\n",
    "\n",
    "    def smooth_l1(self, y_true, y_pred):\n",
    "        absolute_value_loss = K.abs(y_true - y_pred) - 0.5\n",
    "        square_loss = 0.5 * (y_true - y_pred)**2\n",
    "        absolute_value_condition = K.less(absolute_value_loss, 1.0)\n",
    "        l1_smooth_loss = tf.where(absolute_value_condition, square_loss,\n",
    "                                  absolute_value_loss)\n",
    "        return K.sum(l1_smooth_loss, axis=-1)\n",
    "\n",
    "    def cross_entropy(self, y_true, y_pred):\n",
    "        y_pred = K.maximum(K.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "        cross_entropy_loss = - K.sum(y_true * K.log(y_pred), axis=-1)\n",
    "        return cross_entropy_loss\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        class_loss = self.cross_entropy(y_true[:, :, 4:], y_pred[:, :, 4:])\n",
    "        local_loss = self.smooth_l1(y_true[:, :, :4], y_pred[:, :, :4])\n",
    "        negative_mask = y_true[:, :, 4 + self.background_id]\n",
    "        positive_mask = 1 - negative_mask\n",
    "\n",
    "        positive_local_losses = local_loss * positive_mask\n",
    "        positive_class_losses = class_loss * positive_mask\n",
    "        positive_class_loss = K.sum(positive_class_losses, axis=-1)\n",
    "        positive_local_loss = K.sum(positive_local_losses, axis=-1)\n",
    "        \n",
    "        num_positives_per_sample = K.cast(K.sum(positive_mask, -1), 'int32')\n",
    "        num_hard_negatives = self.neg_pos_ratio * num_positives_per_sample\n",
    "        num_negatives_per_sample = K.minimum(num_hard_negatives,\n",
    "                                             self.max_num_negatives)\n",
    "        negative_class_losses = class_loss * negative_mask\n",
    "\n",
    "        elements = (negative_class_losses, num_negatives_per_sample)\n",
    "        negative_class_loss = tf.map_fn(\n",
    "                lambda x: K.sum(tf.nn.top_k(x[0], x[1])[0]),\n",
    "                elements, dtype=tf.float32)\n",
    "\n",
    "        class_loss = positive_class_loss + negative_class_loss\n",
    "        total_loss = class_loss + (self.alpha * positive_local_loss)\n",
    "\n",
    "        batch_mask = K.not_equal(num_positives_per_sample, 0)\n",
    "        total_loss = tf.where(batch_mask, total_loss, K.zeros_like(total_loss))\n",
    "\n",
    "        num_positives_per_sample = tf.where(\n",
    "                batch_mask, num_positives_per_sample,\n",
    "                tf.cast(K.ones_like(num_positives_per_sample), tf.int32))\n",
    "\n",
    "        num_positives_per_sample = K.cast(num_positives_per_sample, 'float32')\n",
    "        total_loss = total_loss / num_positives_per_sample\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "uqwmwHzP3qGy",
    "outputId": "0a938d2b-d656-4fb1-ec9d-a32b190c3a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "vgg = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', \n",
    "                                     input_tensor=None, input_shape=(300, 300, 3), pooling=None, classes=1000)\n",
    "vgg.save_weights('weights_vgg.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAN7s7GSoWmO"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def precompute_priors(image_size=300, feature_map_sizes=[38, 19, 10, 5, 3, 1], steps=[8, 16, 32, 64, 100, 300],\n",
    "                      min_sizes=[30, 60, 111, 162, 213, 264], max_sizes=[60, 111, 162, 213, 264, 315], \n",
    "                      aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], variance=[0.1, 0.2], point_form=False):\n",
    "    boxes = []\n",
    "    for idx, map_size in enumerate(feature_map_sizes):\n",
    "        for x in range(map_size):\n",
    "            for y in range(map_size):\n",
    "                f_k = image_size / steps[idx]\n",
    "                cx = (x + 0.5) / f_k\n",
    "                cy = (y + 0.5) / f_k\n",
    "                s_k = min_sizes[idx] / image_size\n",
    "                boxes = boxes + [cx, cy, s_k, s_k]\n",
    "                s_k_prime = np.sqrt(s_k * (max_sizes[idx] / image_size))\n",
    "                boxes = boxes + [cx, cy, s_k_prime, s_k_prime]\n",
    "                for aspect_ratio in aspect_ratios[idx]:\n",
    "                    boxes = boxes + [cx, cy, s_k * np.sqrt(aspect_ratio), s_k / np.sqrt(aspect_ratio)]\n",
    "                    boxes = boxes + [cx, cy, s_k / np.sqrt(aspect_ratio), s_k * np.sqrt(aspect_ratio)]\n",
    "\n",
    "    boxes = np.asarray(boxes).reshape((-1, 4))\n",
    "    boxes = np.clip(boxes, 0, 1)\n",
    "\n",
    "    if point_form:\n",
    "        return np.concatenate((boxes[:, :2] - boxes[:, 2:]/2, boxes[:, :2] + boxes[:, 2:]/2), 1)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "sKEPtznyUD6g",
    "outputId": "fb2df9ac-7260-45db-f33e-95b6b9e4a439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  drive/My Drive/IUST-DIP/data.zip\n",
      "replace data/dataset.7z? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
      "  inflating: data/dataset.7z         \n",
      "  inflating: data/detections.xlsx    \n",
      "  inflating: data/ground-truth.xlsx  \n",
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 67541227 bytes (65 MiB)\n",
      "\n",
      "Extracting archive: data/dataset.7z\n",
      "--\n",
      "Path = data/dataset.7z\n",
      "Type = 7z\n",
      "Physical Size = 67541227\n",
      "Headers Size = 7942\n",
      "Method = LZMA2:24\n",
      "Solid = +\n",
      "Blocks = 1\n",
      "\n",
      "    \n",
      "Would you like to replace the existing file:\n",
      "  Path:     ./dataset/train/004.txt\n",
      "  Size:     0 bytes\n",
      "  Modified: 2019-12-19 11:46:42\n",
      "with the file from archive:\n",
      "  Path:     dataset/train/004.txt\n",
      "  Size:     0 bytes\n",
      "  Modified: 2019-12-19 11:46:42\n",
      "? (Y)es / (N)o / (A)lways / (S)kip all / A(u)to rename all / (Q)uit? A\n",
      "\n",
      "  0% 3 - dataset/train/004.t                              4% 1        8% 208 - dataset/train/057.j                               15% 231 - dataset/train/088.j                               19% 264 - dataset/train/111.t                               25% 289 - dataset/train/128.j                               30% 325 - dataset/train/168.j                               35% 362 - dataset/train/238.t                               40% 404 - dataset/train/290.j                               46% 435 - dataset/train/341.t                               51% 477 - dataset/train/364.t                               57% 503 - dataset/train/437.j                               62% 542 - dataset/train/494.t                               66% 572 - dataset/train/549.t                               71% 617 - dataset/train/595.t                               77% 658 - dataset/train/634.j                               82% 694 - dataset/valid/083.t                               87% 725 - dataset/valid/127.t                               93% 759 - dataset/valid/283.j                               97% 791 - dataset/valid/463.t                              Everything is Ok\n",
      "\n",
      "Folders: 3\n",
      "Files: 840\n",
      "Size:       80467219\n",
      "Compressed: 67541227\n"
     ]
    }
   ],
   "source": [
    "!unzip drive/My\\ Drive/IUST-DIP/data.zip -d data/\n",
    "!7z x data/dataset.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0ui6LiE7qQc"
   },
   "outputs": [],
   "source": [
    "class DataManager(object):\n",
    "    def __init__(self, dataset_path='dataset', train_val_path=None, mode='train', num_classes=None):\n",
    "        if num_classes is None:\n",
    "            self.num_classes = 2\n",
    "        self.class_keys = np.arange(2)+1  # [1, 2]\n",
    "        self.dataset_path = dataset_path\n",
    "        if train_val_path is None:\n",
    "            self.train_val_path = ['train', 'valid']\n",
    "        self.train_data = dict()  # {'image_name': array(size=(num_objects, 4 + num_classes)}\n",
    "        self.test_data = dict()\n",
    "        self.mode = mode\n",
    "\n",
    "    def _load_data(self):\n",
    "        names = self._load_names()\n",
    "        for name in names:\n",
    "            with open(name + '.txt', 'r') as f:\n",
    "                txt = f.readlines()\n",
    "            num_objects = len(txt)\n",
    "            \n",
    "            bounding_boxes = []\n",
    "            one_hot_classes = []\n",
    "            if num_objects == 0:\n",
    "                annotation = np.zeros((1, 4 + self.num_classes))\n",
    "                if self.mode == 'train':\n",
    "                    self.train_data[name+'.jpg'] = annotation\n",
    "                    continue\n",
    "                elif self.mode == 'test':\n",
    "                    self.test_data[name+'.jpg'] = annotation\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception('wrong mode!')\n",
    "                \n",
    "            for l in txt:\n",
    "                coor_class = l.split(' ')\n",
    "                bounding_boxes.append([int(float(coor)) for coor in coor_class[:-1]])\n",
    "                one_hot_class = [0] * (self.num_classes  + 1)  # add background class with 0 value\n",
    "                one_hot_class[int(float(coor_class[-1]))] = 1\n",
    "                one_hot_classes.append(one_hot_class)\n",
    "            \n",
    "            bounding_boxes = np.asarray(bounding_boxes)\n",
    "            one_hot_classes = np.asarray(one_hot_classes)\n",
    "            annotation = np.hstack((bounding_boxes, one_hot_classes))\n",
    "            if self.mode == 'train':\n",
    "                self.train_data[name+'.jpg'] = annotation\n",
    "            elif self.mode == 'test':\n",
    "                self.test_data[name+'.jpg'] = annotation\n",
    "            else:\n",
    "                raise Exception('wrong mode!')\n",
    "        if self.mode == 'train':\n",
    "            return self.train_data\n",
    "        elif self.mode == 'test':\n",
    "            return self.test_data\n",
    "        else:\n",
    "            raise Exception('wrong mode!')\n",
    "        \n",
    "\n",
    "    def _load_names(self):\n",
    "        import glob\n",
    "        names = []\n",
    "        if self.mode == 'train':\n",
    "            path = self.train_val_path[0]\n",
    "        elif self.mode == 'test':\n",
    "            path = self.train_val_path[1]\n",
    "        else:\n",
    "            raise Exception('Wrong mode!')\n",
    "\n",
    "        for file in glob.glob(self.dataset_path+'/'+path+'/'+'*.jpg'):\n",
    "            names.append(file[:-4]) \n",
    "        return names\n",
    "\n",
    "    def __call__(self):\n",
    "        return self._load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "cUsPjs9v0_iI",
    "outputId": "0fe7793b-9895-4cd9-8749-0669963f1858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/100.jpg\n",
      "[[0. 0. 0. 0. 0. 0.]]\n",
      "dataset/valid/597.jpg\n",
      "[[608  49 682 108   0   1   0]]\n"
     ]
    }
   ],
   "source": [
    "train_data = DataManager()()\n",
    "print(list(train_data.keys())[1])\n",
    "print(list(train_data.values())[1])\n",
    "\n",
    "test_data = DataManager(mode='test')()\n",
    "print(list(test_data.keys())[1])\n",
    "print(list(test_data.values())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3XTFhGrslzV"
   },
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, boxes=None, labels=None):\n",
    "        for t in self.transforms:\n",
    "            img, boxes, labels = t(img, boxes, labels)\n",
    "        return img, boxes, labels\n",
    "\n",
    "\n",
    "class ToFloat(object):\n",
    "    def __init__(self, d_type=np.float32):\n",
    "        self.d_type = d_type\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        return image.astype(self.d_type), boxes.astype(self.d_type), labels.astype(self.d_type)\n",
    "\n",
    "class ZeroMean(object):\n",
    "    def __init__(self, mean, d_type=np.float32):\n",
    "        self.mean = np.array(mean, dtype=d_type)\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        image -= self.mean\n",
    "        return image, boxes, labels\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size=300):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        image = cv2.resize(image, (self.size, self.size))\n",
    "        return image, boxes, labels\n",
    "\n",
    "class ToPercentCoords(object):\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        height, width, channels = image.shape\n",
    "        boxes[:, 0] /= width\n",
    "        boxes[:, 2] /= width\n",
    "        boxes[:, 1] /= height\n",
    "        boxes[:, 3] /= height\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n",
    "class Augmentation(object):\n",
    "    def __init__(self, mode='train', size=300, mean=(104, 117, 123)):\n",
    "        self.mean = mean\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.augment = Compose([\n",
    "                ToFloat(),    \n",
    "                ToPercentCoords(),\n",
    "                Resize(self.size),\n",
    "                ZeroMean(self.mean)\n",
    "            ])\n",
    "\n",
    "        elif self.mode == 'test':\n",
    "            self.augment = Compose([\n",
    "                ToFloat(),\n",
    "                Resize(self.size),\n",
    "                ZeroMean(self.mean)\n",
    "            ])\n",
    "\n",
    "        else:\n",
    "            raise Exception('Invalid mode!')\n",
    "\n",
    "    def __call__(self, img, boxes, labels):\n",
    "        return self.augment(img, boxes, labels)\n",
    "\n",
    "ssd_train_augmentation = Augmentation(mode='train')\n",
    "ssd_test_augmentation = Augmentation(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWrZjuCUBy9G"
   },
   "outputs": [],
   "source": [
    "# taking from https://github.com/fchollet/keras/issues/1638\n",
    "import threading\n",
    "\n",
    "\n",
    "class threadsafe_iterator:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.iterator)\n",
    "\n",
    "\n",
    "def threadsafe_generator(generator):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def wrapped_generator(*args, **kwargs):\n",
    "        return threadsafe_iterator(generator(*args, **kwargs))\n",
    "    return wrapped_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdxHZPuuPsuX"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    def __init__(self, data, prior_boxes, mode='train', transform=None, shuffle=True,\n",
    "                 batch_size=2, num_classes=3, box_scale_factors=[.1, .1, .2, .2]):\n",
    "        self.data = data\n",
    "        self.num_classes = num_classes\n",
    "        self.prior_boxes = prior_boxes\n",
    "        self.transform = transform\n",
    "        self.box_scale_factors = box_scale_factors\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    @threadsafe_generator\n",
    "    def flow(self):       \n",
    "        keys = list(self.data.keys())\n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(keys)\n",
    "\n",
    "        while True:\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            for img in keys:\n",
    "                img_data = cv2.imread(img).copy()\n",
    "                box_data = self.data[img].copy()\n",
    "\n",
    "                data = (img_data, box_data[:, :4], box_data[:, 4:])\n",
    "                img_data, box_corners, labels = self.transform(*data)\n",
    "                box_data = np.concatenate([box_corners, labels], axis=1)\n",
    "\n",
    "#                 box_data = match(self.prior_boxes, box_data,\n",
    "#                                               self.num_classes,\n",
    "#                                               self.box_scale_factors)\n",
    "\n",
    "                inputs.append(img_data)\n",
    "                targets.append(box_data)\n",
    "                if len(targets) == self.batch_size:\n",
    "                    inputs = np.asarray(inputs)\n",
    "                    targets = np.asarray(targets)\n",
    "                    yield (inputs, targets)\n",
    "                    inputs = []\n",
    "                    targets = []\n",
    "\n",
    "train_data_gen = DataGenerator(data=train_data, prior_boxes=precompute_priors(point_form=True), shuffle=False,\n",
    "                               transform=ssd_train_augmentation, batch_size=1)\n",
    "\n",
    "batch = train_data_gen.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DMlJbLuq98y"
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "batch_size = 2\n",
    "num_epochs = 250\n",
    "alpha_loss = 1.0\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 5e-4\n",
    "gamma_decay = 0.1\n",
    "negative_positive_ratio = 3\n",
    "num_classes = 3\n",
    "base_weights_path = 'weights_vgg.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7iBkBlpsLDuH"
   },
   "outputs": [],
   "source": [
    "model = SSD(n_classes=num_classes)\n",
    "model.load_weights(base_weights_path, by_name=True)\n",
    "prior_boxes = precompute_priors(point_form=True)\n",
    "criterion = MultiboxLoss(num_classes, negative_positive_ratio, alpha_loss)\n",
    "optimizer = Adam(lr=learning_rate, decay=weight_decay)\n",
    "model.compile(optimizer, loss=criterion)\n",
    "train_data_generator = DataGenerator(data=train_data, prior_boxes=precompute_priors(point_form=True),\n",
    "                                     mode='train', transform=ssd_train_augmentation,\n",
    "                                     batch_size=batch_size, num_classes=num_classes)\n",
    "\n",
    "test_data_generator = DataGenerator(data=test_data, prior_boxes=precompute_priors(point_form=True),\n",
    "                                     mode='test', transform=ssd_test_augmentation,\n",
    "                                     batch_size=batch_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2q_ntdKM56h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "model_name = 'ssd300'\n",
    "model_path = 'trained_models/' + model_name + '/'\n",
    "save_path = model_path + 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "log = CSVLogger(model_path + model_name + '.log')\n",
    "checkpoint = ModelCheckpoint(save_path, verbose=1, save_weights_only=True)\n",
    "# reduce_on_plateau = ReduceLROnPlateau(factor=gamma_decay, verbose=1)\n",
    "# scheduler = LearningRateManager(learning_rate, gamma_decay, scheduled_epochs)\n",
    "# learning_rate_schedule = LearningRateScheduler(scheduler, verbose=1)\n",
    "callbacks = [checkpoint, log]\n",
    "# callbacks = [checkpoint, log, reduce_on_plateau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tfbw-YIl8duq",
    "outputId": "160abf1a-72a7-4534-b265-05790b1a1337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 21s 131ms/step - loss: 1047.0767 - val_loss: 2.7823\n",
      "\n",
      "Epoch 00001: saving model to trained_models/ssd300/weights.01-2.78.hdf5\n",
      "Epoch 2/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 6.0180 - val_loss: 2.5389\n",
      "\n",
      "Epoch 00002: saving model to trained_models/ssd300/weights.02-2.54.hdf5\n",
      "Epoch 3/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 5.7530 - val_loss: 2.4015\n",
      "\n",
      "Epoch 00003: saving model to trained_models/ssd300/weights.03-2.40.hdf5\n",
      "Epoch 4/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.5912 - val_loss: 2.3044\n",
      "\n",
      "Epoch 00004: saving model to trained_models/ssd300/weights.04-2.30.hdf5\n",
      "Epoch 5/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.4758 - val_loss: 2.2285\n",
      "\n",
      "Epoch 00005: saving model to trained_models/ssd300/weights.05-2.23.hdf5\n",
      "Epoch 6/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.4079 - val_loss: 2.1692\n",
      "\n",
      "Epoch 00006: saving model to trained_models/ssd300/weights.06-2.17.hdf5\n",
      "Epoch 7/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 5.3573 - val_loss: 2.1220\n",
      "\n",
      "Epoch 00007: saving model to trained_models/ssd300/weights.07-2.12.hdf5\n",
      "Epoch 8/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 5.3027 - val_loss: 2.0838\n",
      "\n",
      "Epoch 00008: saving model to trained_models/ssd300/weights.08-2.08.hdf5\n",
      "Epoch 9/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.2535 - val_loss: 2.0498\n",
      "\n",
      "Epoch 00009: saving model to trained_models/ssd300/weights.09-2.05.hdf5\n",
      "Epoch 10/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.2043 - val_loss: 2.0210\n",
      "\n",
      "Epoch 00010: saving model to trained_models/ssd300/weights.10-2.02.hdf5\n",
      "Epoch 11/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.4414 - val_loss: 2.0236\n",
      "\n",
      "Epoch 00011: saving model to trained_models/ssd300/weights.11-2.02.hdf5\n",
      "Epoch 12/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.1871 - val_loss: 1.9936\n",
      "\n",
      "Epoch 00012: saving model to trained_models/ssd300/weights.12-1.99.hdf5\n",
      "Epoch 13/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 5.1641 - val_loss: 1.9722\n",
      "\n",
      "Epoch 00013: saving model to trained_models/ssd300/weights.13-1.97.hdf5\n",
      "Epoch 14/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.1364 - val_loss: 1.9528\n",
      "\n",
      "Epoch 00014: saving model to trained_models/ssd300/weights.14-1.95.hdf5\n",
      "Epoch 15/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 5.1089 - val_loss: 1.9336\n",
      "\n",
      "Epoch 00015: saving model to trained_models/ssd300/weights.15-1.93.hdf5\n",
      "Epoch 16/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 5.0747 - val_loss: 1.9165\n",
      "\n",
      "Epoch 00016: saving model to trained_models/ssd300/weights.16-1.92.hdf5\n",
      "Epoch 17/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 5.0633 - val_loss: 1.9010\n",
      "\n",
      "Epoch 00017: saving model to trained_models/ssd300/weights.17-1.90.hdf5\n",
      "Epoch 18/250\n",
      "158/158 [==============================] - 15s 94ms/step - loss: 5.0456 - val_loss: 1.8872\n",
      "\n",
      "Epoch 00018: saving model to trained_models/ssd300/weights.18-1.89.hdf5\n",
      "Epoch 19/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 5.0244 - val_loss: 1.8741\n",
      "\n",
      "Epoch 00019: saving model to trained_models/ssd300/weights.19-1.87.hdf5\n",
      "Epoch 20/250\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 5.0102 - val_loss: 1.8624\n",
      "\n",
      "Epoch 00020: saving model to trained_models/ssd300/weights.20-1.86.hdf5\n",
      "Epoch 21/250\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 4.9922 - val_loss: 1.8494\n",
      "\n",
      "Epoch 00021: saving model to trained_models/ssd300/weights.21-1.85.hdf5\n",
      "Epoch 22/250\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 4.9838 - val_loss: 1.8389\n",
      "\n",
      "Epoch 00022: saving model to trained_models/ssd300/weights.22-1.84.hdf5\n",
      "Epoch 23/250\n",
      "158/158 [==============================] - 15s 95ms/step - loss: 4.9620 - val_loss: 1.8284\n",
      "\n",
      "Epoch 00023: saving model to trained_models/ssd300/weights.23-1.83.hdf5\n",
      "Epoch 24/250\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 4.9516 - val_loss: 1.8178\n",
      "\n",
      "Epoch 00024: saving model to trained_models/ssd300/weights.24-1.82.hdf5\n",
      "Epoch 25/250\n",
      "158/158 [==============================] - 15s 94ms/step - loss: 4.9356 - val_loss: 1.8078\n",
      "\n",
      "Epoch 00025: saving model to trained_models/ssd300/weights.25-1.81.hdf5\n",
      "Epoch 26/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.9277 - val_loss: 1.7983\n",
      "\n",
      "Epoch 00026: saving model to trained_models/ssd300/weights.26-1.80.hdf5\n",
      "Epoch 27/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.9134 - val_loss: 1.7894\n",
      "\n",
      "Epoch 00027: saving model to trained_models/ssd300/weights.27-1.79.hdf5\n",
      "Epoch 28/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.9019 - val_loss: 1.7792\n",
      "\n",
      "Epoch 00028: saving model to trained_models/ssd300/weights.28-1.78.hdf5\n",
      "Epoch 29/250\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 4.8901 - val_loss: 1.7709\n",
      "\n",
      "Epoch 00029: saving model to trained_models/ssd300/weights.29-1.77.hdf5\n",
      "Epoch 30/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.8824 - val_loss: 1.7619\n",
      "\n",
      "Epoch 00030: saving model to trained_models/ssd300/weights.30-1.76.hdf5\n",
      "Epoch 31/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.8656 - val_loss: 1.7527\n",
      "\n",
      "Epoch 00031: saving model to trained_models/ssd300/weights.31-1.75.hdf5\n",
      "Epoch 32/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.8629 - val_loss: 1.7445\n",
      "\n",
      "Epoch 00032: saving model to trained_models/ssd300/weights.32-1.74.hdf5\n",
      "Epoch 33/250\n",
      "158/158 [==============================] - 15s 94ms/step - loss: 4.8639 - val_loss: 1.7359\n",
      "\n",
      "Epoch 00033: saving model to trained_models/ssd300/weights.33-1.74.hdf5\n",
      "Epoch 34/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.8422 - val_loss: 1.7276\n",
      "\n",
      "Epoch 00034: saving model to trained_models/ssd300/weights.34-1.73.hdf5\n",
      "Epoch 35/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.8313 - val_loss: 1.7189\n",
      "\n",
      "Epoch 00035: saving model to trained_models/ssd300/weights.35-1.72.hdf5\n",
      "Epoch 36/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.8203 - val_loss: 1.7102\n",
      "\n",
      "Epoch 00036: saving model to trained_models/ssd300/weights.36-1.71.hdf5\n",
      "Epoch 37/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.8076 - val_loss: 1.7016\n",
      "\n",
      "Epoch 00037: saving model to trained_models/ssd300/weights.37-1.70.hdf5\n",
      "Epoch 38/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.7969 - val_loss: 1.6938\n",
      "\n",
      "Epoch 00038: saving model to trained_models/ssd300/weights.38-1.69.hdf5\n",
      "Epoch 39/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.7889 - val_loss: 1.6852\n",
      "\n",
      "Epoch 00039: saving model to trained_models/ssd300/weights.39-1.69.hdf5\n",
      "Epoch 40/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.7765 - val_loss: 1.6769\n",
      "\n",
      "Epoch 00040: saving model to trained_models/ssd300/weights.40-1.68.hdf5\n",
      "Epoch 41/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.7753 - val_loss: 1.6688\n",
      "\n",
      "Epoch 00041: saving model to trained_models/ssd300/weights.41-1.67.hdf5\n",
      "Epoch 42/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.7607 - val_loss: 1.6601\n",
      "\n",
      "Epoch 00042: saving model to trained_models/ssd300/weights.42-1.66.hdf5\n",
      "Epoch 43/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.7519 - val_loss: 1.6517\n",
      "\n",
      "Epoch 00043: saving model to trained_models/ssd300/weights.43-1.65.hdf5\n",
      "Epoch 44/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.7404 - val_loss: 1.6434\n",
      "\n",
      "Epoch 00044: saving model to trained_models/ssd300/weights.44-1.64.hdf5\n",
      "Epoch 45/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.7293 - val_loss: 1.6350\n",
      "\n",
      "Epoch 00045: saving model to trained_models/ssd300/weights.45-1.63.hdf5\n",
      "Epoch 46/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.7242 - val_loss: 1.6265\n",
      "\n",
      "Epoch 00046: saving model to trained_models/ssd300/weights.46-1.63.hdf5\n",
      "Epoch 47/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.7107 - val_loss: 1.6179\n",
      "\n",
      "Epoch 00047: saving model to trained_models/ssd300/weights.47-1.62.hdf5\n",
      "Epoch 48/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.7014 - val_loss: 1.6093\n",
      "\n",
      "Epoch 00048: saving model to trained_models/ssd300/weights.48-1.61.hdf5\n",
      "Epoch 49/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.6954 - val_loss: 1.6011\n",
      "\n",
      "Epoch 00049: saving model to trained_models/ssd300/weights.49-1.60.hdf5\n",
      "Epoch 50/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.6793 - val_loss: 1.5918\n",
      "\n",
      "Epoch 00050: saving model to trained_models/ssd300/weights.50-1.59.hdf5\n",
      "Epoch 51/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.6698 - val_loss: 1.5834\n",
      "\n",
      "Epoch 00051: saving model to trained_models/ssd300/weights.51-1.58.hdf5\n",
      "Epoch 52/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.6617 - val_loss: 1.5749\n",
      "\n",
      "Epoch 00052: saving model to trained_models/ssd300/weights.52-1.57.hdf5\n",
      "Epoch 53/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.6482 - val_loss: 1.5654\n",
      "\n",
      "Epoch 00053: saving model to trained_models/ssd300/weights.53-1.57.hdf5\n",
      "Epoch 54/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.6389 - val_loss: 1.5568\n",
      "\n",
      "Epoch 00054: saving model to trained_models/ssd300/weights.54-1.56.hdf5\n",
      "Epoch 55/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.6323 - val_loss: 1.5474\n",
      "\n",
      "Epoch 00055: saving model to trained_models/ssd300/weights.55-1.55.hdf5\n",
      "Epoch 56/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.6160 - val_loss: 1.5377\n",
      "\n",
      "Epoch 00056: saving model to trained_models/ssd300/weights.56-1.54.hdf5\n",
      "Epoch 57/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.6132 - val_loss: 1.5285\n",
      "\n",
      "Epoch 00057: saving model to trained_models/ssd300/weights.57-1.53.hdf5\n",
      "Epoch 58/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.5949 - val_loss: 1.5185\n",
      "\n",
      "Epoch 00058: saving model to trained_models/ssd300/weights.58-1.52.hdf5\n",
      "Epoch 59/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.5833 - val_loss: 1.5095\n",
      "\n",
      "Epoch 00059: saving model to trained_models/ssd300/weights.59-1.51.hdf5\n",
      "Epoch 60/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.5683 - val_loss: 1.4988\n",
      "\n",
      "Epoch 00060: saving model to trained_models/ssd300/weights.60-1.50.hdf5\n",
      "Epoch 61/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.5570 - val_loss: 1.4888\n",
      "\n",
      "Epoch 00061: saving model to trained_models/ssd300/weights.61-1.49.hdf5\n",
      "Epoch 62/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.5589 - val_loss: 1.4790\n",
      "\n",
      "Epoch 00062: saving model to trained_models/ssd300/weights.62-1.48.hdf5\n",
      "Epoch 63/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.5380 - val_loss: 1.4688\n",
      "\n",
      "Epoch 00063: saving model to trained_models/ssd300/weights.63-1.47.hdf5\n",
      "Epoch 64/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.5239 - val_loss: 1.4582\n",
      "\n",
      "Epoch 00064: saving model to trained_models/ssd300/weights.64-1.46.hdf5\n",
      "Epoch 65/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.5140 - val_loss: 1.4477\n",
      "\n",
      "Epoch 00065: saving model to trained_models/ssd300/weights.65-1.45.hdf5\n",
      "Epoch 66/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.4991 - val_loss: 1.4370\n",
      "\n",
      "Epoch 00066: saving model to trained_models/ssd300/weights.66-1.44.hdf5\n",
      "Epoch 67/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.4881 - val_loss: 1.4261\n",
      "\n",
      "Epoch 00067: saving model to trained_models/ssd300/weights.67-1.43.hdf5\n",
      "Epoch 68/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.4787 - val_loss: 1.4150\n",
      "\n",
      "Epoch 00068: saving model to trained_models/ssd300/weights.68-1.41.hdf5\n",
      "Epoch 69/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 4.4650 - val_loss: 1.4038\n",
      "\n",
      "Epoch 00069: saving model to trained_models/ssd300/weights.69-1.40.hdf5\n",
      "Epoch 70/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.4508 - val_loss: 1.3925\n",
      "\n",
      "Epoch 00070: saving model to trained_models/ssd300/weights.70-1.39.hdf5\n",
      "Epoch 71/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.4262 - val_loss: 1.3810\n",
      "\n",
      "Epoch 00071: saving model to trained_models/ssd300/weights.71-1.38.hdf5\n",
      "Epoch 72/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.4103 - val_loss: 1.3692\n",
      "\n",
      "Epoch 00072: saving model to trained_models/ssd300/weights.72-1.37.hdf5\n",
      "Epoch 73/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.3981 - val_loss: 1.3576\n",
      "\n",
      "Epoch 00073: saving model to trained_models/ssd300/weights.73-1.36.hdf5\n",
      "Epoch 74/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.3799 - val_loss: 1.3457\n",
      "\n",
      "Epoch 00074: saving model to trained_models/ssd300/weights.74-1.35.hdf5\n",
      "Epoch 75/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.3587 - val_loss: 1.3331\n",
      "\n",
      "Epoch 00075: saving model to trained_models/ssd300/weights.75-1.33.hdf5\n",
      "Epoch 76/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.3517 - val_loss: 1.3205\n",
      "\n",
      "Epoch 00076: saving model to trained_models/ssd300/weights.76-1.32.hdf5\n",
      "Epoch 77/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.3305 - val_loss: 1.3080\n",
      "\n",
      "Epoch 00077: saving model to trained_models/ssd300/weights.77-1.31.hdf5\n",
      "Epoch 78/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.3093 - val_loss: 1.2952\n",
      "\n",
      "Epoch 00078: saving model to trained_models/ssd300/weights.78-1.30.hdf5\n",
      "Epoch 79/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.2839 - val_loss: 1.2818\n",
      "\n",
      "Epoch 00079: saving model to trained_models/ssd300/weights.79-1.28.hdf5\n",
      "Epoch 80/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.2698 - val_loss: 1.2686\n",
      "\n",
      "Epoch 00080: saving model to trained_models/ssd300/weights.80-1.27.hdf5\n",
      "Epoch 81/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.2463 - val_loss: 1.2552\n",
      "\n",
      "Epoch 00081: saving model to trained_models/ssd300/weights.81-1.26.hdf5\n",
      "Epoch 82/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.2318 - val_loss: 1.2412\n",
      "\n",
      "Epoch 00082: saving model to trained_models/ssd300/weights.82-1.24.hdf5\n",
      "Epoch 83/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 4.2086 - val_loss: 1.2275\n",
      "\n",
      "Epoch 00083: saving model to trained_models/ssd300/weights.83-1.23.hdf5\n",
      "Epoch 84/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.1948 - val_loss: 1.2135\n",
      "\n",
      "Epoch 00084: saving model to trained_models/ssd300/weights.84-1.21.hdf5\n",
      "Epoch 85/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 4.1699 - val_loss: 1.1991\n",
      "\n",
      "Epoch 00085: saving model to trained_models/ssd300/weights.85-1.20.hdf5\n",
      "Epoch 86/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.1509 - val_loss: 1.1846\n",
      "\n",
      "Epoch 00086: saving model to trained_models/ssd300/weights.86-1.18.hdf5\n",
      "Epoch 87/250\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 4.1317 - val_loss: 1.1700\n",
      "\n",
      "Epoch 00087: saving model to trained_models/ssd300/weights.87-1.17.hdf5\n",
      "Epoch 88/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.1146 - val_loss: 1.1553\n",
      "\n",
      "Epoch 00088: saving model to trained_models/ssd300/weights.88-1.16.hdf5\n",
      "Epoch 89/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.0872 - val_loss: 1.1400\n",
      "\n",
      "Epoch 00089: saving model to trained_models/ssd300/weights.89-1.14.hdf5\n",
      "Epoch 90/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.0747 - val_loss: 1.1249\n",
      "\n",
      "Epoch 00090: saving model to trained_models/ssd300/weights.90-1.12.hdf5\n",
      "Epoch 91/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 4.0439 - val_loss: 1.1093\n",
      "\n",
      "Epoch 00091: saving model to trained_models/ssd300/weights.91-1.11.hdf5\n",
      "Epoch 92/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 4.0253 - val_loss: 1.0942\n",
      "\n",
      "Epoch 00092: saving model to trained_models/ssd300/weights.92-1.09.hdf5\n",
      "Epoch 93/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.9944 - val_loss: 1.0784\n",
      "\n",
      "Epoch 00093: saving model to trained_models/ssd300/weights.93-1.08.hdf5\n",
      "Epoch 94/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.9808 - val_loss: 1.0628\n",
      "\n",
      "Epoch 00094: saving model to trained_models/ssd300/weights.94-1.06.hdf5\n",
      "Epoch 95/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 3.9565 - val_loss: 1.0470\n",
      "\n",
      "Epoch 00095: saving model to trained_models/ssd300/weights.95-1.05.hdf5\n",
      "Epoch 96/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.9355 - val_loss: 1.0311\n",
      "\n",
      "Epoch 00096: saving model to trained_models/ssd300/weights.96-1.03.hdf5\n",
      "Epoch 97/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.9049 - val_loss: 1.0145\n",
      "\n",
      "Epoch 00097: saving model to trained_models/ssd300/weights.97-1.01.hdf5\n",
      "Epoch 98/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.8870 - val_loss: 0.9983\n",
      "\n",
      "Epoch 00098: saving model to trained_models/ssd300/weights.98-1.00.hdf5\n",
      "Epoch 99/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.8562 - val_loss: 0.9819\n",
      "\n",
      "Epoch 00099: saving model to trained_models/ssd300/weights.99-0.98.hdf5\n",
      "Epoch 100/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 3.8357 - val_loss: 0.9653\n",
      "\n",
      "Epoch 00100: saving model to trained_models/ssd300/weights.100-0.97.hdf5\n",
      "Epoch 101/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.8022 - val_loss: 0.9488\n",
      "\n",
      "Epoch 00101: saving model to trained_models/ssd300/weights.101-0.95.hdf5\n",
      "Epoch 102/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.7738 - val_loss: 0.9321\n",
      "\n",
      "Epoch 00102: saving model to trained_models/ssd300/weights.102-0.93.hdf5\n",
      "Epoch 103/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.7610 - val_loss: 0.9154\n",
      "\n",
      "Epoch 00103: saving model to trained_models/ssd300/weights.103-0.92.hdf5\n",
      "Epoch 104/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.7465 - val_loss: 0.8987\n",
      "\n",
      "Epoch 00104: saving model to trained_models/ssd300/weights.104-0.90.hdf5\n",
      "Epoch 105/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.7250 - val_loss: 0.8821\n",
      "\n",
      "Epoch 00105: saving model to trained_models/ssd300/weights.105-0.88.hdf5\n",
      "Epoch 106/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.6832 - val_loss: 0.8651\n",
      "\n",
      "Epoch 00106: saving model to trained_models/ssd300/weights.106-0.87.hdf5\n",
      "Epoch 107/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.6456 - val_loss: 0.8483\n",
      "\n",
      "Epoch 00107: saving model to trained_models/ssd300/weights.107-0.85.hdf5\n",
      "Epoch 108/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.6121 - val_loss: 0.8313\n",
      "\n",
      "Epoch 00108: saving model to trained_models/ssd300/weights.108-0.83.hdf5\n",
      "Epoch 109/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.5816 - val_loss: 0.8147\n",
      "\n",
      "Epoch 00109: saving model to trained_models/ssd300/weights.109-0.81.hdf5\n",
      "Epoch 110/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.5591 - val_loss: 0.7979\n",
      "\n",
      "Epoch 00110: saving model to trained_models/ssd300/weights.110-0.80.hdf5\n",
      "Epoch 111/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.5286 - val_loss: 0.7811\n",
      "\n",
      "Epoch 00111: saving model to trained_models/ssd300/weights.111-0.78.hdf5\n",
      "Epoch 112/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.5105 - val_loss: 0.7647\n",
      "\n",
      "Epoch 00112: saving model to trained_models/ssd300/weights.112-0.76.hdf5\n",
      "Epoch 113/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 3.5033 - val_loss: 0.7481\n",
      "\n",
      "Epoch 00113: saving model to trained_models/ssd300/weights.113-0.75.hdf5\n",
      "Epoch 114/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.4585 - val_loss: 0.7315\n",
      "\n",
      "Epoch 00114: saving model to trained_models/ssd300/weights.114-0.73.hdf5\n",
      "Epoch 115/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.4260 - val_loss: 0.7150\n",
      "\n",
      "Epoch 00115: saving model to trained_models/ssd300/weights.115-0.71.hdf5\n",
      "Epoch 116/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.3982 - val_loss: 0.6989\n",
      "\n",
      "Epoch 00116: saving model to trained_models/ssd300/weights.116-0.70.hdf5\n",
      "Epoch 117/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.3546 - val_loss: 0.6833\n",
      "\n",
      "Epoch 00117: saving model to trained_models/ssd300/weights.117-0.68.hdf5\n",
      "Epoch 118/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.3308 - val_loss: 0.6673\n",
      "\n",
      "Epoch 00118: saving model to trained_models/ssd300/weights.118-0.67.hdf5\n",
      "Epoch 119/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.3057 - val_loss: 0.6513\n",
      "\n",
      "Epoch 00119: saving model to trained_models/ssd300/weights.119-0.65.hdf5\n",
      "Epoch 120/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.2961 - val_loss: 0.6360\n",
      "\n",
      "Epoch 00120: saving model to trained_models/ssd300/weights.120-0.64.hdf5\n",
      "Epoch 121/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.2494 - val_loss: 0.6205\n",
      "\n",
      "Epoch 00121: saving model to trained_models/ssd300/weights.121-0.62.hdf5\n",
      "Epoch 122/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.2248 - val_loss: 0.6053\n",
      "\n",
      "Epoch 00122: saving model to trained_models/ssd300/weights.122-0.61.hdf5\n",
      "Epoch 123/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 3.1750 - val_loss: 0.5901\n",
      "\n",
      "Epoch 00123: saving model to trained_models/ssd300/weights.123-0.59.hdf5\n",
      "Epoch 124/250\n",
      "158/158 [==============================] - 16s 100ms/step - loss: 3.1561 - val_loss: 0.5753\n",
      "\n",
      "Epoch 00124: saving model to trained_models/ssd300/weights.124-0.58.hdf5\n",
      "Epoch 125/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.1333 - val_loss: 0.5607\n",
      "\n",
      "Epoch 00125: saving model to trained_models/ssd300/weights.125-0.56.hdf5\n",
      "Epoch 126/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.1035 - val_loss: 0.5461\n",
      "\n",
      "Epoch 00126: saving model to trained_models/ssd300/weights.126-0.55.hdf5\n",
      "Epoch 127/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 3.0811 - val_loss: 0.5321\n",
      "\n",
      "Epoch 00127: saving model to trained_models/ssd300/weights.127-0.53.hdf5\n",
      "Epoch 128/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 3.0270 - val_loss: 0.5180\n",
      "\n",
      "Epoch 00128: saving model to trained_models/ssd300/weights.128-0.52.hdf5\n",
      "Epoch 129/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.9771 - val_loss: 0.5043\n",
      "\n",
      "Epoch 00129: saving model to trained_models/ssd300/weights.129-0.50.hdf5\n",
      "Epoch 130/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.9559 - val_loss: 0.4912\n",
      "\n",
      "Epoch 00130: saving model to trained_models/ssd300/weights.130-0.49.hdf5\n",
      "Epoch 131/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.9269 - val_loss: 0.4778\n",
      "\n",
      "Epoch 00131: saving model to trained_models/ssd300/weights.131-0.48.hdf5\n",
      "Epoch 132/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.9005 - val_loss: 0.4652\n",
      "\n",
      "Epoch 00132: saving model to trained_models/ssd300/weights.132-0.47.hdf5\n",
      "Epoch 133/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.9191 - val_loss: 0.4530\n",
      "\n",
      "Epoch 00133: saving model to trained_models/ssd300/weights.133-0.45.hdf5\n",
      "Epoch 134/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.8541 - val_loss: 0.4406\n",
      "\n",
      "Epoch 00134: saving model to trained_models/ssd300/weights.134-0.44.hdf5\n",
      "Epoch 135/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.8038 - val_loss: 0.4284\n",
      "\n",
      "Epoch 00135: saving model to trained_models/ssd300/weights.135-0.43.hdf5\n",
      "Epoch 136/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.7795 - val_loss: 0.4170\n",
      "\n",
      "Epoch 00136: saving model to trained_models/ssd300/weights.136-0.42.hdf5\n",
      "Epoch 137/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 2.7604 - val_loss: 0.4058\n",
      "\n",
      "Epoch 00137: saving model to trained_models/ssd300/weights.137-0.41.hdf5\n",
      "Epoch 138/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 2.7301 - val_loss: 0.3953\n",
      "\n",
      "Epoch 00138: saving model to trained_models/ssd300/weights.138-0.40.hdf5\n",
      "Epoch 139/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.7091 - val_loss: 0.3844\n",
      "\n",
      "Epoch 00139: saving model to trained_models/ssd300/weights.139-0.38.hdf5\n",
      "Epoch 140/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 2.6837 - val_loss: 0.3741\n",
      "\n",
      "Epoch 00140: saving model to trained_models/ssd300/weights.140-0.37.hdf5\n",
      "Epoch 141/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.6482 - val_loss: 0.3640\n",
      "\n",
      "Epoch 00141: saving model to trained_models/ssd300/weights.141-0.36.hdf5\n",
      "Epoch 142/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.6399 - val_loss: 0.3548\n",
      "\n",
      "Epoch 00142: saving model to trained_models/ssd300/weights.142-0.35.hdf5\n",
      "Epoch 143/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 2.6332 - val_loss: 0.3458\n",
      "\n",
      "Epoch 00143: saving model to trained_models/ssd300/weights.143-0.35.hdf5\n",
      "Epoch 144/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.6015 - val_loss: 0.3370\n",
      "\n",
      "Epoch 00144: saving model to trained_models/ssd300/weights.144-0.34.hdf5\n",
      "Epoch 145/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 2.5590 - val_loss: 0.3286\n",
      "\n",
      "Epoch 00145: saving model to trained_models/ssd300/weights.145-0.33.hdf5\n",
      "Epoch 146/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 2.5309 - val_loss: 0.3206\n",
      "\n",
      "Epoch 00146: saving model to trained_models/ssd300/weights.146-0.32.hdf5\n",
      "Epoch 147/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.5068 - val_loss: 0.3133\n",
      "\n",
      "Epoch 00147: saving model to trained_models/ssd300/weights.147-0.31.hdf5\n",
      "Epoch 148/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.4600 - val_loss: 0.3060\n",
      "\n",
      "Epoch 00148: saving model to trained_models/ssd300/weights.148-0.31.hdf5\n",
      "Epoch 149/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.4420 - val_loss: 0.2991\n",
      "\n",
      "Epoch 00149: saving model to trained_models/ssd300/weights.149-0.30.hdf5\n",
      "Epoch 150/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.4516 - val_loss: 0.2927\n",
      "\n",
      "Epoch 00150: saving model to trained_models/ssd300/weights.150-0.29.hdf5\n",
      "Epoch 151/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.4349 - val_loss: 0.2868\n",
      "\n",
      "Epoch 00151: saving model to trained_models/ssd300/weights.151-0.29.hdf5\n",
      "Epoch 152/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 2.3981 - val_loss: 0.2811\n",
      "\n",
      "Epoch 00152: saving model to trained_models/ssd300/weights.152-0.28.hdf5\n",
      "Epoch 153/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 2.3886 - val_loss: 0.2758\n",
      "\n",
      "Epoch 00153: saving model to trained_models/ssd300/weights.153-0.28.hdf5\n",
      "Epoch 154/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 2.3822 - val_loss: 0.2711\n",
      "\n",
      "Epoch 00154: saving model to trained_models/ssd300/weights.154-0.27.hdf5\n",
      "Epoch 155/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.3932 - val_loss: 0.2666\n",
      "\n",
      "Epoch 00155: saving model to trained_models/ssd300/weights.155-0.27.hdf5\n",
      "Epoch 156/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.3534 - val_loss: 0.2629\n",
      "\n",
      "Epoch 00156: saving model to trained_models/ssd300/weights.156-0.26.hdf5\n",
      "Epoch 157/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.3107 - val_loss: 0.2587\n",
      "\n",
      "Epoch 00157: saving model to trained_models/ssd300/weights.157-0.26.hdf5\n",
      "Epoch 158/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 2.2919 - val_loss: 0.2549\n",
      "\n",
      "Epoch 00158: saving model to trained_models/ssd300/weights.158-0.25.hdf5\n",
      "Epoch 159/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 2.2877 - val_loss: 0.2513\n",
      "\n",
      "Epoch 00159: saving model to trained_models/ssd300/weights.159-0.25.hdf5\n",
      "Epoch 160/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.2714 - val_loss: 0.2484\n",
      "\n",
      "Epoch 00160: saving model to trained_models/ssd300/weights.160-0.25.hdf5\n",
      "Epoch 161/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.2706 - val_loss: 0.2454\n",
      "\n",
      "Epoch 00161: saving model to trained_models/ssd300/weights.161-0.25.hdf5\n",
      "Epoch 162/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.2530 - val_loss: 0.2429\n",
      "\n",
      "Epoch 00162: saving model to trained_models/ssd300/weights.162-0.24.hdf5\n",
      "Epoch 163/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.2490 - val_loss: 0.2405\n",
      "\n",
      "Epoch 00163: saving model to trained_models/ssd300/weights.163-0.24.hdf5\n",
      "Epoch 164/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.2226 - val_loss: 0.2382\n",
      "\n",
      "Epoch 00164: saving model to trained_models/ssd300/weights.164-0.24.hdf5\n",
      "Epoch 165/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.2207 - val_loss: 0.2362\n",
      "\n",
      "Epoch 00165: saving model to trained_models/ssd300/weights.165-0.24.hdf5\n",
      "Epoch 166/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.2008 - val_loss: 0.2343\n",
      "\n",
      "Epoch 00166: saving model to trained_models/ssd300/weights.166-0.23.hdf5\n",
      "Epoch 167/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.1552 - val_loss: 0.2323\n",
      "\n",
      "Epoch 00167: saving model to trained_models/ssd300/weights.167-0.23.hdf5\n",
      "Epoch 168/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.1546 - val_loss: 0.2308\n",
      "\n",
      "Epoch 00168: saving model to trained_models/ssd300/weights.168-0.23.hdf5\n",
      "Epoch 169/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.1255 - val_loss: 0.2295\n",
      "\n",
      "Epoch 00169: saving model to trained_models/ssd300/weights.169-0.23.hdf5\n",
      "Epoch 170/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.1189 - val_loss: 0.2284\n",
      "\n",
      "Epoch 00170: saving model to trained_models/ssd300/weights.170-0.23.hdf5\n",
      "Epoch 171/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.1243 - val_loss: 0.2270\n",
      "\n",
      "Epoch 00171: saving model to trained_models/ssd300/weights.171-0.23.hdf5\n",
      "Epoch 172/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 2.0986 - val_loss: 0.2261\n",
      "\n",
      "Epoch 00172: saving model to trained_models/ssd300/weights.172-0.23.hdf5\n",
      "Epoch 173/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.0983 - val_loss: 0.2252\n",
      "\n",
      "Epoch 00173: saving model to trained_models/ssd300/weights.173-0.23.hdf5\n",
      "Epoch 174/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.1041 - val_loss: 0.2243\n",
      "\n",
      "Epoch 00174: saving model to trained_models/ssd300/weights.174-0.22.hdf5\n",
      "Epoch 175/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.0659 - val_loss: 0.2237\n",
      "\n",
      "Epoch 00175: saving model to trained_models/ssd300/weights.175-0.22.hdf5\n",
      "Epoch 176/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.0567 - val_loss: 0.2230\n",
      "\n",
      "Epoch 00176: saving model to trained_models/ssd300/weights.176-0.22.hdf5\n",
      "Epoch 177/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.0565 - val_loss: 0.2225\n",
      "\n",
      "Epoch 00177: saving model to trained_models/ssd300/weights.177-0.22.hdf5\n",
      "Epoch 178/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 2.0353 - val_loss: 0.2221\n",
      "\n",
      "Epoch 00178: saving model to trained_models/ssd300/weights.178-0.22.hdf5\n",
      "Epoch 179/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.0251 - val_loss: 0.2217\n",
      "\n",
      "Epoch 00179: saving model to trained_models/ssd300/weights.179-0.22.hdf5\n",
      "Epoch 180/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 2.0145 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00180: saving model to trained_models/ssd300/weights.180-0.22.hdf5\n",
      "Epoch 181/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 2.0030 - val_loss: 0.2208\n",
      "\n",
      "Epoch 00181: saving model to trained_models/ssd300/weights.181-0.22.hdf5\n",
      "Epoch 182/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9904 - val_loss: 0.2205\n",
      "\n",
      "Epoch 00182: saving model to trained_models/ssd300/weights.182-0.22.hdf5\n",
      "Epoch 183/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9882 - val_loss: 0.2203\n",
      "\n",
      "Epoch 00183: saving model to trained_models/ssd300/weights.183-0.22.hdf5\n",
      "Epoch 184/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9569 - val_loss: 0.2196\n",
      "\n",
      "Epoch 00184: saving model to trained_models/ssd300/weights.184-0.22.hdf5\n",
      "Epoch 185/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9482 - val_loss: 0.2194\n",
      "\n",
      "Epoch 00185: saving model to trained_models/ssd300/weights.185-0.22.hdf5\n",
      "Epoch 186/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9486 - val_loss: 0.2191\n",
      "\n",
      "Epoch 00186: saving model to trained_models/ssd300/weights.186-0.22.hdf5\n",
      "Epoch 187/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9428 - val_loss: 0.2192\n",
      "\n",
      "Epoch 00187: saving model to trained_models/ssd300/weights.187-0.22.hdf5\n",
      "Epoch 188/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9636 - val_loss: 0.2189\n",
      "\n",
      "Epoch 00188: saving model to trained_models/ssd300/weights.188-0.22.hdf5\n",
      "Epoch 189/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.9321 - val_loss: 0.2194\n",
      "\n",
      "Epoch 00189: saving model to trained_models/ssd300/weights.189-0.22.hdf5\n",
      "Epoch 190/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.9080 - val_loss: 0.2191\n",
      "\n",
      "Epoch 00190: saving model to trained_models/ssd300/weights.190-0.22.hdf5\n",
      "Epoch 191/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.8826 - val_loss: 0.2189\n",
      "\n",
      "Epoch 00191: saving model to trained_models/ssd300/weights.191-0.22.hdf5\n",
      "Epoch 192/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.8705 - val_loss: 0.2187\n",
      "\n",
      "Epoch 00192: saving model to trained_models/ssd300/weights.192-0.22.hdf5\n",
      "Epoch 193/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.8711 - val_loss: 0.2185\n",
      "\n",
      "Epoch 00193: saving model to trained_models/ssd300/weights.193-0.22.hdf5\n",
      "Epoch 194/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.8658 - val_loss: 0.2184\n",
      "\n",
      "Epoch 00194: saving model to trained_models/ssd300/weights.194-0.22.hdf5\n",
      "Epoch 195/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.8516 - val_loss: 0.2182\n",
      "\n",
      "Epoch 00195: saving model to trained_models/ssd300/weights.195-0.22.hdf5\n",
      "Epoch 196/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 1.8454 - val_loss: 0.2182\n",
      "\n",
      "Epoch 00196: saving model to trained_models/ssd300/weights.196-0.22.hdf5\n",
      "Epoch 197/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.8469 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00197: saving model to trained_models/ssd300/weights.197-0.22.hdf5\n",
      "Epoch 198/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.8162 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00198: saving model to trained_models/ssd300/weights.198-0.22.hdf5\n",
      "Epoch 199/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.7895 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00199: saving model to trained_models/ssd300/weights.199-0.22.hdf5\n",
      "Epoch 200/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.7719 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00200: saving model to trained_models/ssd300/weights.200-0.22.hdf5\n",
      "Epoch 201/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.7677 - val_loss: 0.2180\n",
      "\n",
      "Epoch 00201: saving model to trained_models/ssd300/weights.201-0.22.hdf5\n",
      "Epoch 202/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.7681 - val_loss: 0.2178\n",
      "\n",
      "Epoch 00202: saving model to trained_models/ssd300/weights.202-0.22.hdf5\n",
      "Epoch 203/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 1.7897 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00203: saving model to trained_models/ssd300/weights.203-0.22.hdf5\n",
      "Epoch 204/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.7754 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00204: saving model to trained_models/ssd300/weights.204-0.22.hdf5\n",
      "Epoch 205/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 1.7699 - val_loss: 0.2177\n",
      "\n",
      "Epoch 00205: saving model to trained_models/ssd300/weights.205-0.22.hdf5\n",
      "Epoch 206/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.7326 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00206: saving model to trained_models/ssd300/weights.206-0.22.hdf5\n",
      "Epoch 207/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.7144 - val_loss: 0.2176\n",
      "\n",
      "Epoch 00207: saving model to trained_models/ssd300/weights.207-0.22.hdf5\n",
      "Epoch 208/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 1.6950 - val_loss: 0.2177\n",
      "\n",
      "Epoch 00208: saving model to trained_models/ssd300/weights.208-0.22.hdf5\n",
      "Epoch 209/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.7062 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00209: saving model to trained_models/ssd300/weights.209-0.22.hdf5\n",
      "Epoch 210/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 1.7290 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00210: saving model to trained_models/ssd300/weights.210-0.22.hdf5\n",
      "Epoch 211/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.6856 - val_loss: 0.2180\n",
      "\n",
      "Epoch 00211: saving model to trained_models/ssd300/weights.211-0.22.hdf5\n",
      "Epoch 212/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.6503 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00212: saving model to trained_models/ssd300/weights.212-0.22.hdf5\n",
      "Epoch 213/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.6524 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00213: saving model to trained_models/ssd300/weights.213-0.22.hdf5\n",
      "Epoch 214/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.6476 - val_loss: 0.2178\n",
      "\n",
      "Epoch 00214: saving model to trained_models/ssd300/weights.214-0.22.hdf5\n",
      "Epoch 215/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.6448 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00215: saving model to trained_models/ssd300/weights.215-0.22.hdf5\n",
      "Epoch 216/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.6345 - val_loss: 0.2180\n",
      "\n",
      "Epoch 00216: saving model to trained_models/ssd300/weights.216-0.22.hdf5\n",
      "Epoch 217/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.6269 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00217: saving model to trained_models/ssd300/weights.217-0.22.hdf5\n",
      "Epoch 218/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.6478 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00218: saving model to trained_models/ssd300/weights.218-0.22.hdf5\n",
      "Epoch 219/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.6305 - val_loss: 0.2177\n",
      "\n",
      "Epoch 00219: saving model to trained_models/ssd300/weights.219-0.22.hdf5\n",
      "Epoch 220/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.6228 - val_loss: 0.2180\n",
      "\n",
      "Epoch 00220: saving model to trained_models/ssd300/weights.220-0.22.hdf5\n",
      "Epoch 221/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 1.5767 - val_loss: 0.2180\n",
      "\n",
      "Epoch 00221: saving model to trained_models/ssd300/weights.221-0.22.hdf5\n",
      "Epoch 222/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 1.5544 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00222: saving model to trained_models/ssd300/weights.222-0.22.hdf5\n",
      "Epoch 223/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.5421 - val_loss: 0.2180\n",
      "\n",
      "Epoch 00223: saving model to trained_models/ssd300/weights.223-0.22.hdf5\n",
      "Epoch 224/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.5401 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00224: saving model to trained_models/ssd300/weights.224-0.22.hdf5\n",
      "Epoch 225/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.5369 - val_loss: 0.2180\n",
      "\n",
      "Epoch 00225: saving model to trained_models/ssd300/weights.225-0.22.hdf5\n",
      "Epoch 226/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.5332 - val_loss: 0.2179\n",
      "\n",
      "Epoch 00226: saving model to trained_models/ssd300/weights.226-0.22.hdf5\n",
      "Epoch 227/250\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 1.5250 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00227: saving model to trained_models/ssd300/weights.227-0.22.hdf5\n",
      "Epoch 228/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.5070 - val_loss: 0.2182\n",
      "\n",
      "Epoch 00228: saving model to trained_models/ssd300/weights.228-0.22.hdf5\n",
      "Epoch 229/250\n",
      "158/158 [==============================] - 14s 92ms/step - loss: 1.5407 - val_loss: 0.2182\n",
      "\n",
      "Epoch 00229: saving model to trained_models/ssd300/weights.229-0.22.hdf5\n",
      "Epoch 230/250\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 1.5116 - val_loss: 0.2184\n",
      "\n",
      "Epoch 00230: saving model to trained_models/ssd300/weights.230-0.22.hdf5\n",
      "Epoch 231/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.4598 - val_loss: 0.2185\n",
      "\n",
      "Epoch 00231: saving model to trained_models/ssd300/weights.231-0.22.hdf5\n",
      "Epoch 232/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.4532 - val_loss: 0.2185\n",
      "\n",
      "Epoch 00232: saving model to trained_models/ssd300/weights.232-0.22.hdf5\n",
      "Epoch 233/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.4517 - val_loss: 0.2185\n",
      "\n",
      "Epoch 00233: saving model to trained_models/ssd300/weights.233-0.22.hdf5\n",
      "Epoch 234/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.4372 - val_loss: 0.2185\n",
      "\n",
      "Epoch 00234: saving model to trained_models/ssd300/weights.234-0.22.hdf5\n",
      "Epoch 235/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.4621 - val_loss: 0.2186\n",
      "\n",
      "Epoch 00235: saving model to trained_models/ssd300/weights.235-0.22.hdf5\n",
      "Epoch 236/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.4493 - val_loss: 0.2187\n",
      "\n",
      "Epoch 00236: saving model to trained_models/ssd300/weights.236-0.22.hdf5\n",
      "Epoch 237/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.4648 - val_loss: 0.2188\n",
      "\n",
      "Epoch 00237: saving model to trained_models/ssd300/weights.237-0.22.hdf5\n",
      "Epoch 238/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.4312 - val_loss: 0.2191\n",
      "\n",
      "Epoch 00238: saving model to trained_models/ssd300/weights.238-0.22.hdf5\n",
      "Epoch 239/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.3913 - val_loss: 0.2191\n",
      "\n",
      "Epoch 00239: saving model to trained_models/ssd300/weights.239-0.22.hdf5\n",
      "Epoch 240/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 1.3726 - val_loss: 0.2194\n",
      "\n",
      "Epoch 00240: saving model to trained_models/ssd300/weights.240-0.22.hdf5\n",
      "Epoch 241/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.3723 - val_loss: 0.2195\n",
      "\n",
      "Epoch 00241: saving model to trained_models/ssd300/weights.241-0.22.hdf5\n",
      "Epoch 242/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.3704 - val_loss: 0.2198\n",
      "\n",
      "Epoch 00242: saving model to trained_models/ssd300/weights.242-0.22.hdf5\n",
      "Epoch 243/250\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 1.3905 - val_loss: 0.2201\n",
      "\n",
      "Epoch 00243: saving model to trained_models/ssd300/weights.243-0.22.hdf5\n",
      "Epoch 244/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.3896 - val_loss: 0.2201\n",
      "\n",
      "Epoch 00244: saving model to trained_models/ssd300/weights.244-0.22.hdf5\n",
      "Epoch 245/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 1.3476 - val_loss: 0.2204\n",
      "\n",
      "Epoch 00245: saving model to trained_models/ssd300/weights.245-0.22.hdf5\n",
      "Epoch 246/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 1.3133 - val_loss: 0.2202\n",
      "\n",
      "Epoch 00246: saving model to trained_models/ssd300/weights.246-0.22.hdf5\n",
      "Epoch 247/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.2972 - val_loss: 0.2202\n",
      "\n",
      "Epoch 00247: saving model to trained_models/ssd300/weights.247-0.22.hdf5\n",
      "Epoch 248/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.2770 - val_loss: 0.2203\n",
      "\n",
      "Epoch 00248: saving model to trained_models/ssd300/weights.248-0.22.hdf5\n",
      "Epoch 249/250\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 1.2978 - val_loss: 0.2205\n",
      "\n",
      "Epoch 00249: saving model to trained_models/ssd300/weights.249-0.22.hdf5\n",
      "Epoch 250/250\n",
      "158/158 [==============================] - 14s 89ms/step - loss: 1.2911 - val_loss: 0.2206\n",
      "\n",
      "Epoch 00250: saving model to trained_models/ssd300/weights.250-0.22.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a9a1e1f28>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator.flow(),\n",
    "                    steps_per_epoch=int(len(train_data) / batch_size),\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=test_data_generator.flow(),\n",
    "                    validation_steps=int(len(test_data) / batch_size),\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DUrgn4-qPPg0",
    "outputId": "5b5f7ff3-e94d-4a88-ca2b-452c4d2c9d4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1qplmVDF16ahIprrl2sPK9sAd8p1TnUjQ'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !git clone https://gist.github.com/dc7e60aa487430ea704a8cb3f2c5d6a6.git /tmp/colab_util_repo\n",
    "# !mv /tmp/colab_util_repo/colab_util.py colab_util.py \n",
    "# !rm -r /tmp/colab_util_repo\n",
    "# from colab_util import *\n",
    "# drive_handler = GoogleDriveHandler()\n",
    "# drive_handler.upload('trained_models/ssd300/weights.250-0.22.hdf5', parent_path='IUST-DIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YYxFHaTBFHAF"
   },
   "source": [
    "## README\n",
    "\n",
    "ALL parts except following ones has been implemented:\n",
    "1. Match function: to match precomputed prior boxes with the ones model has predicted\n",
    "2. NMS: Non-max suppression for prediciton\n",
    "3. Negative Mining: Reducing number of negative samples w.r.t. positive ones\n",
    "\n",
    "Parts Have been IMPLEMENTED:\n",
    "1. Model definition\n",
    "2. Loss function\n",
    "3. Data Generator\n",
    "4. Augmentation\n",
    "5. Transforms\n",
    "6. Prior Box computations\n",
    "\n",
    "I will finish it after final exams, but at the time of deadline, I had many other daily life (SURVIVAL) problems to deal.\n",
    "Thank you for your time."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DIP-HW12",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
